{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpPHz2Lp6VD"
      },
      "source": [
        "# IMA 206 - Coding autoencoders in Pytorch\n",
        "\n",
        "The lab was originally created by Alasdair Newson (https://sites.google.com/site/alasdairnewson/)\n",
        "\n",
        "The current version is made by Loic Le Folgoc. If you have questions, please contact me at loic dot lefolgoc at telecom-paris dot fr.\n",
        "\n",
        "## Objective:\n",
        "\n",
        "The goal of this TP is to explore autoencoders and variational autoencoders applied to a simple dataset. In this first part, we will look at an autoencoder applied to MNIST. We recall that an autoencoder is a neural network with the following general architecture:\n",
        "\n",
        "\n",
        "![AUTOENCODER](https://drive.google.com/uc?id=11dfNujSHa2-_eThp2aTpL1M_hLaEQX-G)\n",
        "\n",
        "\n",
        "The tensor $z$ in the middle of the network is called a __latent code__, and it belongs to the latent space. It is this latent space which is interesting in autoencoders (for image synthesis, editing, etc).\n",
        "\n",
        "## Your task:\n",
        "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE or # FILL IN CODE or ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First of all, let's load some packages:"
      ],
      "metadata": {
        "id": "gp13aVUQq1WX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "JqNeIJ8Op8Ao"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import pdb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "U6NKzRPlDKZp"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hyj5dj_eui9D"
      },
      "source": [
        "First, we load the mnist dataset. I find that training on the full training dataset `mnist_trainset` is fast enough even on CPU (5-10 minutes), but should you need it, we create a reduced trainset below.\n",
        "\n",
        "Feel free to train on `mnist_trainset_reduced` instead if you prefer (results might be of poorer quality). To do so, replace the argument `mnist_trainset` in the `torch.utils.data.DataLoader(...)` call creating `mnist_train_loader` in the cell below by `mnist_trainset_reduced` (and same for `mnist_testset` and `mnist_testset_reduced`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "4YPLKlPrufSk"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# MNIST Dataset\n",
        "mnist_trainset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "mnist_testset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "#create data loader with smaller dataset size\n",
        "max_mnist_size = 5000\n",
        "mnist_trainset_reduced = torch.utils.data.random_split(mnist_trainset, [max_mnist_size, len(mnist_trainset)-max_mnist_size])[0]\n",
        "mnist_train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "# download test dataset\n",
        "max_mnist_size = 1000\n",
        "mnist_testset_reduced = torch.utils.data.random_split(mnist_testset, [max_mnist_size, len(mnist_testset)-max_mnist_size])[0]\n",
        "mnist_test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=batch_size, shuffle=False, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7YhlBT2PN9I",
        "outputId": "41162c39-6d84-47e2-93a0-23cb6629566b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "mnist_trainset.data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-bkK4ktwfvC"
      },
      "source": [
        "# 1. Vanilla Autoencoder\n",
        "\n",
        "Now, we define our autoencoder model. The autoencoder class `AEModel` is made of an `Encoder` and a `Decoder`, which we create first. We will reuse the `Encoder` and `Decoder` classes when building our variational autoencoder model, and wrap them in a `VAEModel` instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jLa2-jQwxSI"
      },
      "source": [
        "We will use the following convolutional architectures :\n",
        "\n",
        "__Encoder__ :\n",
        "- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=1; + ReLU\n",
        "- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=0; + ReLU\n",
        "- Conv layer, 32 filters, 4x4 kernel, stride=2, padding=0; + ReLU\n",
        "- Flatten\n",
        "- Dense layer with 64 output neurons; + ReLU\n",
        "- Dense layer with `self.latent_dim*self.multiplier` output neurons.\n",
        "\n",
        "For the autoencoder, `self.multiplier=1` as the encoder outputs a `self.latent_dim`-dimensional latent code. For the variational autoencoder, we will set `self.multiplier=2` as the encoder will output `self.latent_dim`-dimensional mean and log-variance parameters of the Gaussian distribution $q_\\phi(z|x)$.\n",
        "\n",
        "__Decoder__ (the decoder of the AE and VAE are the same, they always outputs an image/probability map, given a code $z$ as input):\n",
        "- Dense layer with 64 output neurons; + ReLU\n",
        "- Dense layer with ??? output neurons; + ReLU\n",
        "- Reshape, to a `(C, H, W)` tensor with `C=32`, `H=???`, `W=???`.\n",
        "- Conv transpose layer, 32 filters, 4x4 kernel, stride=2, padding=0; +ReLU\n",
        "- Conv transpose layer, 32 filters, 4x4 kernel, stride=2, padding=0; +ReLU\n",
        "- Conv transpose layer, 1 filter, 4x4 kernel, stride=2, padding=1; +Sigmoid\n",
        "\n",
        "The number of output neurons of the second dense layer is exactly the number of input neurons in the first dense layer of the encoder (i.e., the number of values in the feature maps of the conv layer immediately before it).\n",
        "\n",
        "For the reshape operations, use the ```A.view(dim_1,dim_2,...)``` function, where ```A``` is your tensor."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Hint for computing the number of neurons that are not given to you__: This [great resource](https://madebyollin.github.io/convnet-calculator/) lets you compute the size of the output tensor following any convolution layer depending on the input tensor shape and conv parameters."
      ],
      "metadata": {
        "id": "eNH7ScylwKa-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(torch.nn.Module):\n",
        "    def __init__(self, latent_dim=10, multiplier=1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.multiplier = multiplier\n",
        "        self.reshape = (32, 2, 2)  # C x H x W after last conv layer\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=0)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=0)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.lin1 = nn.Linear(in_features=32*2*2, out_features=64)  # Flatten 32 * 2 * 2\n",
        "        self.lin2 = nn.Linear(in_features=64, out_features=self.latent_dim * self.multiplier)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # Apply convolutional layers with ReLU\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "\n",
        "        # Flatten the output of conv3\n",
        "        x = x.view(batch_size, -1)  # Flatten into (B, 32*2*2)\n",
        "\n",
        "        # Apply first dense layer with ReLU\n",
        "        x = F.relu(self.lin1(x))\n",
        "\n",
        "        # Final dense layer for latent code\n",
        "        x = self.lin2(x)\n",
        "\n",
        "        if self.multiplier == 1:\n",
        "            return x.view(batch_size, self.latent_dim)  # Reshape if needed to match the output dimensions\n",
        "        else:\n",
        "            return x.view(batch_size, self.latent_dim, self.multiplier)  # Reshape for VAE\n",
        "\n"
      ],
      "metadata": {
        "id": "0-IyPUQkIR-U"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, latent_dim=10):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.reshape = (32, 2, 2)  # Shape to start the transpose convolutions\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.lin1 = nn.Linear(in_features=latent_dim, out_features=64)\n",
        "        self.lin2 = nn.Linear(in_features=64, out_features=32*2*2)\n",
        "\n",
        "        # Convolutional layers\n",
        "        self.convT1 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=0)\n",
        "        self.convT2 = nn.ConvTranspose2d(in_channels=32, out_channels=32, kernel_size=4, stride=2, padding=0)\n",
        "        self.convT3 = nn.ConvTranspose2d(in_channels=32, out_channels=1, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        batch_size = z.size(0)\n",
        "\n",
        "        # Fully connected layers with ReLu activations\n",
        "        z = F.relu(self.lin1(z))\n",
        "        z = F.relu(self.lin2(z))\n",
        "\n",
        "        # Reshape\n",
        "        z = z.view(batch_size, *self.reshape)  # Reshape into the form before the first transposed conv layer\n",
        "\n",
        "        # Convolutional layers with ReLu activations\n",
        "        z = F.relu(self.convT1(z))\n",
        "        z = F.relu(self.convT2(z))\n",
        "\n",
        "        # Final conv layer with sigmoid activation\n",
        "        z = torch.sigmoid(self.convT3(z))\n",
        "\n",
        "        return z\n"
      ],
      "metadata": {
        "id": "U64AqYvpYpr6"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "latent_dim = 10\n",
        "decoder = Decoder(latent_dim)\n",
        "\n",
        "# Create a dummy latent vector\n",
        "dummy_latent_vector = torch.randn(batch_size, latent_dim)\n",
        "output = decoder(dummy_latent_vector)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "id": "HOY03y7Wqysd",
        "outputId": "a085442e-ae6c-4839-897e-6c413d2eedba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The autoencoder model itself is basically a wrapper around an `Encoder` and a `Decoder`. In the forward pass, the input images contained in the tensor `x` are passed through the `Encoder` to obtain the latent codes `z` then these codes are fed to the `Decoder` to produce the reconstructions `y`."
      ],
      "metadata": {
        "id": "JmGyzVp4HVRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AEModel(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        \"\"\"\n",
        "        Class which defines model and forward pass.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        latent_dim : int\n",
        "            Dimensionality of latent code.\n",
        "        \"\"\"\n",
        "        super(AEModel, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        # Initialize encoder and decoder with the given latent dimension\n",
        "        self.encoder = Encoder(latent_dim=latent_dim)\n",
        "        self.decoder = Decoder(latent_dim=latent_dim)\n",
        "\n",
        "    def forward(self, x, mode='sample'):\n",
        "        \"\"\"\n",
        "        Forward pass of model, used for training or reconstruction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
        "\n",
        "        Outputs a dictionary containing:\n",
        "          codes - the latent codes corresponding to the input images\n",
        "          reconstructions - the images reconstructed by the autoencoder\n",
        "        \"\"\"\n",
        "\n",
        "        # z is the output of the encoder\n",
        "        z = self.encoder(x)\n",
        "\n",
        "        # Decode the samples to image space\n",
        "        y = self.decoder(z)\n",
        "\n",
        "        # Return everything:\n",
        "        return {\n",
        "            'reconstructions': y,\n",
        "            'codes': z\n",
        "            }\n"
      ],
      "metadata": {
        "id": "T9MYicH9Zf3o"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we carefully create the reconstruction loss. It will be reused for the VAE loss later on."
      ],
      "metadata": {
        "id": "VeWePstgIVBw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The reconstruction loss translates a pixel-wise Bernoulli probabilistic model into a loss (`F.binary_cross_entropy`). It takes input images `data` and reconstructed probability maps `reconstructions` and computes the binary cross-entropy, from the two images."
      ],
      "metadata": {
        "id": "3FIb4BDrInYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def reconstruction_loss(reconstructions, data):\n",
        "    \"\"\"\n",
        "    Calculates the reconstruction loss for a batch of data. I.e., negative\n",
        "    log likelihood.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : torch.Tensor\n",
        "        Input data (e.g. batch of images). Shape : (batch_size, 1, height, width).\n",
        "    reconstructions : torch.Tensor\n",
        "        Reconstructed data. Shape : (batch_size, 1, height, width).\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss : torch.Tensor\n",
        "        Binary cross entropy, AVERAGED over images in the batch but SUMMED over\n",
        "        pixel and channel.\n",
        "    \"\"\"\n",
        "    batch_size, n_chan, height, width = reconstructions.size()\n",
        "\n",
        "    # The pixel-wise loss is the binary cross-entropy, computed from\n",
        "    # reconstructions and data. Here we need to sum across dimensions of each image,\n",
        "    # but average the sum across the batch.\n",
        "    # The parameter reduction='none' returns the loss for each element in the batch without any reduction.\n",
        "    loss = F.binary_cross_entropy(reconstructions, data, reduction='none')\n",
        "\n",
        "    # Sum the loss over all dimensions except the batch dimension (dim=0)\n",
        "    loss = loss.view(batch_size, -1).sum(dim=1).mean()  # sum over all pixels and channels, then average over batch\n",
        "\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "CWrQbhOnbv7y"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the vanilla autoencoder"
      ],
      "metadata": {
        "id": "iqibefFRJHy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The training proceeds as usual. We instantiate a model, move it to the correct device, create an optimizer and write the training loop."
      ],
      "metadata": {
        "id": "RQbHTXMLJODw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters\n",
        "latent_dim = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "n_epoch = 5 # if running on GPU you can use more epochs (10 or more)"
      ],
      "metadata": {
        "id": "NyZcTZP3a_kc"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ft_3txj0bZjO",
        "outputId": "615c61bc-af38-4178-8b6a-984398012f38"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "oV40vRMQRoG1"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "ae_model = AEModel(latent_dim)\n",
        "ae_model = ae_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the AdamW optimizer, set the correct learning rate and weight_decay to 1e-4\n",
        "optimizer = optim.AdamW(ae_model.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "sbnKjRTDaynz"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ae_model.train()\n",
        "\n",
        "for epoch in range(0,n_epoch):\n",
        "  train_loss=0.0\n",
        "\n",
        "  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n",
        "      for data, labels in tepoch:\n",
        "        tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        # Move data to the correct device\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Zero the gradients carried over from previous steps\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass the input data through the model\n",
        "        predict = ae_model(data)\n",
        "        reconstructions = predict['reconstructions']\n",
        "\n",
        "        # Compute the AE loss\n",
        "        loss = reconstruction_loss(reconstructions, data)\n",
        "\n",
        "        # Backpropagate the loss, update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Aggregate the training loss for display at the end of the epoch\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Update tqdm bar to show the current loss\n",
        "        tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(mnist_train_loader)))"
      ],
      "metadata": {
        "id": "S2-phJydcICh",
        "outputId": "8ff142fe-f153-471f-ea2b-0a06937c0431",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 938/938 [00:59<00:00, 15.73batch/s, loss=94.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss: 107.5782\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 938/938 [00:59<00:00, 15.85batch/s, loss=105]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 95.4294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 938/938 [00:58<00:00, 16.01batch/s, loss=77.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 90.6988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 938/938 [00:59<00:00, 15.81batch/s, loss=83.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 88.3327\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 938/938 [00:59<00:00, 15.78batch/s, loss=91.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 86.8535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the vanilla autoencoder"
      ],
      "metadata": {
        "id": "iHscBN4KJ2Sq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3EbmswSzJdK"
      },
      "source": [
        "We define functions for qualitative testing of the autoencoder model. We will reuse them throughout the lab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "T8jXjdRyzMy2"
      },
      "outputs": [],
      "source": [
        "def display_images(imgs):\n",
        "  '''\n",
        "  Display a batch of images (typically synthetic/generated images)\n",
        "  '''\n",
        "  r = 1\n",
        "  c = imgs.shape[0]\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    # black and white images\n",
        "    axs[j].imshow(imgs[j, 0,:,:].detach().cpu().numpy(), cmap='gray')\n",
        "    axs[j].axis('off')\n",
        "  plt.show()\n",
        "\n",
        "def display_ae_images(ae_model, test_imgs):\n",
        "  '''\n",
        "  Display a batch of input images along with their reconstructions by a given model\n",
        "    First row: input images\n",
        "    Second row: reconstructed images\n",
        "  '''\n",
        "  n_images = 5\n",
        "  idx = np.random.randint(0, test_imgs.shape[0], n_images)\n",
        "  test_imgs = test_imgs[idx,:,:,:]\n",
        "\n",
        "  # get output images\n",
        "  output_imgs = ae_model(test_imgs.to(ae_model.encoder.conv1.weight.device))['reconstructions']\n",
        "  output_imgs = output_imgs.detach().cpu().numpy()\n",
        "\n",
        "  r = 2\n",
        "  c = n_images\n",
        "  fig, axs = plt.subplots(r, c)\n",
        "  for j in range(c):\n",
        "    axs[0,j].imshow(test_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[0,j].axis('off')\n",
        "    axs[1,j].imshow(output_imgs[j, 0,:,:], cmap='gray')\n",
        "    axs[1,j].axis('off')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how well the autoencoder reconstructs images from the training set:"
      ],
      "metadata": {
        "id": "TQiEplCiKnlT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "9pbXch29d68D",
        "outputId": "23c3a71d-e29e-48fb-a45f-c4cac01e952d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# reconstructing training images\n",
        "train_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(ae_model, train_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "What about images from the test set?"
      ],
      "metadata": {
        "id": "h3XxWKjGKtoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reconstructing test images\n",
        "test_imgs = next(iter(mnist_test_loader))[0]\n",
        "display_ae_images(ae_model, test_imgs)"
      ],
      "metadata": {
        "id": "ExMLThLofLn2",
        "outputId": "6d9b7a07-7f8e-4857-a179-0bbaf40ed8bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is not too much overfitting at work here apparently. We can quantify this by computing the reconstruction loss over the test dataset (below) and comparing it to the reconstruction loss over the training dataset at the end of training (check the training cell above)."
      ],
      "metadata": {
        "id": "icWAOpmWKwid"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ae_model.eval()\n",
        "test_loss=0.0\n",
        "\n",
        "# We will store all the latent codes corresponding to the test images for reuse\n",
        "# later on.\n",
        "zs_test = np.zeros((len(mnist_test_loader.dataset),ae_model.latent_dim))\n",
        "\n",
        "n = 0\n",
        "with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for data, labels in tepoch:\n",
        "    # Put the data on the correct device:\n",
        "    data = data.to(device)\n",
        "\n",
        "    # Pass the data through the model\n",
        "    predict = ae_model(data)\n",
        "    reconstructions = predict['reconstructions']\n",
        "    z = predict['codes']\n",
        "\n",
        "    # Compute the AE loss\n",
        "    loss = reconstruction_loss(reconstructions, data)\n",
        "\n",
        "    # Store quantities of interest\n",
        "    minibatch_size = z.shape[0]\n",
        "    zs_test[n:(n+minibatch_size),:] = z.detach().cpu().numpy()\n",
        "\n",
        "    # Compute the loss\n",
        "    test_loss += loss.item()\n",
        "\n",
        "    # tqdm bar displays the loss\n",
        "    tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    # increment n to fill next parts of the arrays\n",
        "    n += minibatch_size\n",
        "\n",
        "print('Test Loss: {:.4f}'.format(test_loss/len(mnist_test_loader)))"
      ],
      "metadata": {
        "id": "HbyzjCqUjHQU",
        "outputId": "ecaba2c1-7091-44a2-f219-44be0a1da6ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 27.13batch/s, loss=82.2]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 85.7555\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assPaJqB5sa-"
      },
      "source": [
        "The test and training average reconstruction losses are indeed similar."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are you happy with the quality of the __reconstructions__? Next, we will see if this autoencoder model is good at __generating__ images."
      ],
      "metadata": {
        "id": "SoLTYl8ELZsj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "The autoencoder shows strong performance in generalization, evidenced by consistent reconstruction losses across training and test data, indicating no significant overfitting. For qualitative evaluation, visual inspection of reconstructions is crucial to confirm if the model accurately captures important image features.\n",
        "\n",
        "Regarding image generation, traditional autoencoders like yours are limited compared to specialized models such as Variational Autoencoders (VAEs) or Generative Adversarial Networks (GANs), which are better suited for generating new images. Exploring latent space interpolations can provide insights into how the model transitions between learned features, but for generating new images, consider transitioning to a model designed for generative tasks."
      ],
      "metadata": {
        "id": "cERH5MvoBgit"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO9ATQEiyr3b"
      },
      "source": [
        "# 2. Image generation with the vanilla autoencoder\n",
        "\n",
        "Unfortunately, the vanilla autoencoder is not in itself a generative model because it does not define a joint probability distribution of the data and latent codes. We need to come up with roundabout ways to synthetize data based on this model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we consider two naïve approaches to creating generative models from the AE. The general idea is the following:\n",
        "\n",
        "- train an autoencoder\n",
        "- estimate different statistics (mean, variance) of the data in the latent space\n",
        "- using these statistics, define a model based on a Gaussian distribution in the latent space\n",
        "- generate latent codes with this distribution, then decode them back to image space to obtain synthetic images\n",
        "\n",
        "We will consider these two situations :\n",
        "\n",
        "- a multivariate Gaussian distribution with __diagonal covariance matrix__ (each latent variable is an independent random variable). This requires the mean and variance in each latent variable;\n",
        "- a multivariate Gaussian distribution with __non-diagonal covariance matrix__. This requires the mean and covariance matrix of the latent codes.\n",
        "\n",
        "Obviously, since this is done _a posteriori_ after training the autoencoder, there is nothing which guarantees that the latent codes do indeed follow a Gaussian distribution. Our goal will be to verify that Variational Autoencoders indeed produce better results than such naïve approaches."
      ],
      "metadata": {
        "id": "ECzGyqbyMaCt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2M1-BRmf56d"
      },
      "source": [
        "### 2.0. Defining and generating random Gaussian latent codes\n",
        "\n",
        "Let $z$ be a latent code and $D$ the dimension of the latent space (called ``latent_dim`` in the code). We suppose that the $z$'s follow a multivariate Gaussian distribution, written as:\n",
        "\n",
        "\\begin{equation}\n",
        "z \\sim \\mathcal{N}\\left(\n",
        "\\mu,\n",
        "\\bf{C}\n",
        "\\right),\n",
        "\\end{equation}\n",
        "where $\\mu$ and $\\bf{C}$ are the mean vector and covariance matrix of the Gaussian distribution. To define such a generative model, we must therefore determine $\\mu$ and $\\bf{C}$. Once this is done, we can generate a random Gaussian latent code in the following manner:\n",
        "\n",
        "\\begin{equation}\n",
        "z = \\mu + {\\bf{L}} \\varepsilon,\n",
        "\\end{equation}\n",
        "where $\\varepsilon$ is a random vector drawn from a multivariate normal distribution ($\\mu=0$ and ${\\bf{C}} = \\text{Id}$), and $\\bf{L}$ is output by a Cholesky decomposition of the positive semi-definite covariance matrix. In other words:\n",
        "\n",
        "\\begin{equation}\n",
        "{\\bf{C}} = {\\bf{L}}{\\bf{L}^T}.\n",
        "\\end{equation}\n",
        "\n",
        "This gives a simple method of producing a multivariate Gaussian random variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWpucm972i7j"
      },
      "source": [
        "### 2.1. A Gaussian model with diagonal covariance\n",
        "\n",
        "The first naïve model is  defined in this first case as:\n",
        "\n",
        "- $\\bf{\\mu}=\\left[\\mu_0, \\mu_1, \\cdots, \\mu_{d-1}\\right]^T$\n",
        "- $\n",
        "  \\bf{C} = \\begin{pmatrix}\n",
        "\\sigma_0^2 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_1^2 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_{d-1}^2\n",
        "\\end{pmatrix}$\n",
        "\n",
        "In this situation, therefore, the matrix $\\bf{L}$ can be calculated quite simply, as:\n",
        "- $\n",
        "  \\bf{L} = \\begin{pmatrix}\n",
        "\\sigma_0 & 0 & \\cdots & 0 \\\\\n",
        "0 & \\sigma_1 & \\cdots & 0 \\\\\n",
        "\\vdots & \\ddots & \\ddots & \\vdots \\\\\n",
        "0 & 0 & \\cdots & \\sigma_{d-1}\n",
        "\\end{pmatrix}$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to compute the mean and the component-wise standard deviations from a batch of data. For simplicity you are going to use the latent codes `zs_test` corresponding to the test data to estimate these quantities.<br>\n",
        "\n",
        "It is actually bad practice, and it would be better to estimate them from the training dataset. We do not do so here for convenience because we have already computed `zs_test` above (we have verified above that overfitting was not a problem, so the difference between the two estimates should be minor)."
      ],
      "metadata": {
        "id": "gZCIY7RsN4NY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "sUXHCtvW2iQ0",
        "outputId": "d7a7ed2a-3aff-4074-9bd1-4573af0ceac3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average of latent codes: [-8.85468533 15.17751306 17.01929926 11.58433965 -3.13572592  6.38032318\n",
            " -8.10075021 -0.12056771  4.12286407  6.8625337 ]\n",
            "Standard deviation of latent codes: [5.88665917 5.26157446 5.59998321 5.54143984 6.40677863 5.728305\n",
            " 6.27025392 6.03492374 4.92420857 5.50876205]\n"
          ]
        }
      ],
      "source": [
        "# zs_test is of shape (N,D) where N is the test dataset size and D the latent dimension\n",
        "# Compute the vector of mean values and the vector of component-wise std's.\n",
        "z_average = np.mean(zs_test, axis=0)\n",
        "z_sigma = np.std(zs_test, axis=0)\n",
        "\n",
        "print(\"Average of latent codes:\",z_average)\n",
        "print(\"Standard deviation of latent codes:\",z_sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lrpc62ML9K4l"
      },
      "source": [
        "Now, in the next cell generate data with this simple generative model using the approach described above. Display these images with the `display_images` function.\n",
        "\n",
        "__Hint__. You do not actually have to define the matrix $\\bf{L}$ in this case, an element-wise multiplication of two (properly chosen) vectors will suffice. To generate multivariate normal random variables you can use the following Pytorch function:\n",
        "\n",
        "- `torch.randn`\n",
        "\n",
        "To convert a numpy array to pytorch tensor, use `torch.from_numpy(...).float()`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images = 5):\n",
        "    # Sample noise from a standard Gaussian distribution\n",
        "    epsilon = torch.randn(n_images, len(z_average))\n",
        "\n",
        "    # Convert mean and std dev from numpy arrays to torch tensors\n",
        "    z_average = torch.from_numpy(z_average).float().to(device)\n",
        "    z_sigma = torch.from_numpy(z_sigma).float().to(device)\n",
        "\n",
        "    # Using epsilon, generate samples from N(mu,C)\n",
        "    z_generated = z_average + z_sigma * epsilon\n",
        "\n",
        "    # Decode back to image space\n",
        "    imgs_generated = ae_model.decoder(z_generated.to(device))\n",
        "\n",
        "    return imgs_generated"
      ],
      "metadata": {
        "id": "cRXyUkVeppif"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "1_Tekii-9QEo",
        "outputId": "39671caa-4c33-4e12-b071-460b694bb91a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "imgs_generated = generate_images_diagonal_gaussian(ae_model, z_average, z_sigma, n_images=5)\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiNaEgLIloeA"
      },
      "source": [
        "What do you think of these samples? Next let's try a slightly more sophisticated model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "The main issues with the generated images using the simple Gaussian model with diagonal covariance in the vanilla autoencoder were poor visual quality and a lack of diversity among the images. The visual quality issue relates to the images not being crisp or detailed enough, often appearing blurry or distorted, which makes them not easily recognizable as MNIST digits. The lack of diversity means the generated images look too similar to each other, indicating that the model's latent space may not be capturing the full complexity and variability of the dataset. These issues suggest that the model's current approach to sampling and reconstructing images from the latent space is too simplistic and fails to adequately model the underlying data distribution."
      ],
      "metadata": {
        "id": "E5f1k60tJO3W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjVPfkRKYMSh"
      },
      "source": [
        "### 2.2. Non-diagonal Gaussian model\n",
        "\n",
        "The second model uses a non-diagonal covariance matrix $\\bf{C}$ in the multivariate Gaussian distribution. In the next cell, calculate the mean and covariance matrix from `zs_test`.\n",
        "\n",
        "__Hint__. You can use the `np.cov` function. Make sure to put the data in the right format for this. Print the shape of z_covariance to verify that you have a matrix of the correct shape (the covariance matrix and not the Gram matrix)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "ArXgre39CD2H",
        "outputId": "cd92a574-1a18-42b2-f1dd-5a5c7770a1ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average of latent codes: [-8.85468533 15.17751306 17.01929926 11.58433965 -3.13572592  6.38032318\n",
            " -8.10075021 -0.12056771  4.12286407  6.8625337 ]\n",
            "Covariance matrix of latent codes: [[ 34.65622185  -2.3458894    4.97472187   4.96496511 -13.70362242\n",
            "  -13.89412914   0.87368037   4.33836289   9.65479222  -5.96452637]\n",
            " [ -2.3458894   27.68693447   4.0521347    3.19249659   2.65650213\n",
            "    1.97665947   8.04953757   7.60374308   2.83279537  -4.30234782]\n",
            " [  4.97472187   4.0521347   31.36294827   0.0507858   -4.9441055\n",
            "    0.15652958  -8.87755231  -4.52451079   4.73846331  -4.41356582]\n",
            " [  4.96496511   3.19249659   0.0507858   30.71062659  -9.94824638\n",
            "   -0.22789847  -8.29080525   3.94942053  -4.55463986  -5.02000363]\n",
            " [-13.70362242   2.65650213  -4.9441055   -9.94824638  41.05091747\n",
            "   -5.81350489   8.42850968 -15.81509577  -7.94425797  11.61443449]\n",
            " [-13.89412914   1.97665947   0.15652958  -0.22789847  -5.81350489\n",
            "   32.81675985  -9.21972314  15.88606972   3.88591165  -7.22017182]\n",
            " [  0.87368037   8.04953757  -8.87755231  -8.29080525   8.42850968\n",
            "   -9.21972314  39.32001624   3.11437706  -1.51723479   8.05876936]\n",
            " [  4.33836289   7.60374308  -4.52451079   3.94942053 -15.81509577\n",
            "   15.88606972   3.11437706  36.42394699   5.04802049 -20.57855388]\n",
            " [  9.65479222   2.83279537   4.73846331  -4.55463986  -7.94425797\n",
            "    3.88591165  -1.51723479   5.04802049  24.25025507  -3.10347962]\n",
            " [ -5.96452637  -4.30234782  -4.41356582  -5.02000363  11.61443449\n",
            "   -7.22017182   8.05876936 -20.57855388  -3.10347962  30.34949426]]\n"
          ]
        }
      ],
      "source": [
        "z_average = np.mean(zs_test, axis=0)\n",
        "z_covariance = np.cov(zs_test, rowvar=False)\n",
        "\n",
        "print(\"Average of latent codes:\", z_average)\n",
        "print(\"Covariance matrix of latent codes:\", z_covariance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhXU8cnTZ0E8"
      },
      "source": [
        "Now, generate some samples with this distribution. In this case, you actually have to calculate the Cholesky decomposition and find $\\bf{L}$. For this, you can use `np.linalg.cholesky`. Then compute the latent codes according to $z = \\mu + {\\bf{L}} \\varepsilon$.\n",
        "\n",
        "__Hint__. You can use `torch.matmul`. Pay attention to the dimension of `epsilon` to implement it correctly."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate Cholesky decomposition of covariance matrix : C = L L^T\n",
        "L = np.linalg.cholesky(z_covariance)"
      ],
      "metadata": {
        "id": "rSbYZLZGrcdR"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "zXGlJTZ7Z4ed"
      },
      "outputs": [],
      "source": [
        "def generate_images_non_diagonal_gaussian(ae_model, z_average, L, n_images=5):\n",
        "    # Generate noise according to a standard Gaussian distribution\n",
        "    epsilon = torch.randn(n_images, L.shape[0]).to(device)\n",
        "\n",
        "    # Convert the average and Cholesky decomposition to PyTorch tensors\n",
        "    z_average = torch.from_numpy(z_average).float().to(device)\n",
        "    L = torch.from_numpy(L).float().to(device)\n",
        "\n",
        "    # Sample latent codes using epsilon\n",
        "    # The matrix multiplication transforms epsilon into the distribution defined by the mean and covariance\n",
        "    z_generated = z_average + torch.matmul(epsilon, L.t())\n",
        "\n",
        "    # Decode back to image space\n",
        "    imgs_generated = ae_model.decoder(z_generated.to(device))\n",
        "\n",
        "    return imgs_generated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate images using this model now:"
      ],
      "metadata": {
        "id": "xs0o_VUvR3jl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_generated = generate_images_non_diagonal_gaussian(ae_model, z_average, L, n_images = 5)\n",
        "display_images(imgs_generated)"
      ],
      "metadata": {
        "id": "kGCJVjbXsKLE",
        "outputId": "e5d9175d-4935-4eb3-b588-378f08064b67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLtsdri6zKEm"
      },
      "source": [
        "You should see some improvement, but we can do better than this. Thus, we turn to the variational autoencoder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UqeNhuSdnDt"
      },
      "source": [
        "# 3. Variational autoencoder\n",
        "\n",
        "Now, we are going to create a variational autoencoder to carry out __image generation__. Let's first recall the idea of a variational autoencoder.\n",
        "\n",
        "### Main idea\n",
        "\n",
        "The main idea is to create an autoencoder whose latent codes follow a certain distribution (a Gaussian distribution in practice). This is done with two tools :\n",
        "\n",
        "- A specific architecture, where the encoder produces the mean and variance of the latent codes\n",
        "- A specially designed loss function\n",
        "\n",
        "Once the VAE is trained, it is possible to sample in the latent space by producing random normal variables and simply decoding.\n",
        "\n",
        "### Architecture\n",
        "\n",
        "The architecture of the VAE model is the same as before (using `Encoder` with `multiplier=2` and `Decoder`). However the wrapper `VAEModel` will be a bit more complex as we need to implement the reparametrization trick. We will also implement the code to generate samples (for test time).\n",
        "\n",
        "### Variational Autoencoder loss\n",
        "\n",
        "The VAE loss consists in a reconstruction loss and a KL divergence term.\n",
        "\n",
        "- The reconstruction loss is the same `reconstruction_loss` as before. In other words, the reconstructions are compared to the input images using binary cross-entropy. The reconstructions are generated by sampling a latent code from $q(z|x)$ and decoding it back to image space.\n",
        "\n",
        "- You will implement the KL divergence term manually below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "6siMHQLheM4T"
      },
      "outputs": [],
      "source": [
        "class VAEModel(nn.Module):\n",
        "    def __init__(self, latent_dim):\n",
        "        super(VAEModel, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        # Encoder and Decoder Initialization\n",
        "        self.encoder = Encoder(latent_dim=latent_dim, multiplier=2)  # 'multiplier=2' for mean and log-var\n",
        "        self.decoder = Decoder(latent_dim=latent_dim)\n",
        "\n",
        "    def reparameterize(self, mean, logvar, mode='sample'):\n",
        "        \"\"\"\n",
        "        Samples from a normal distribution using the reparameterization trick.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : torch.Tensor\n",
        "            Mean of the normal distribution. Shape (batch_size, latent_dim)\n",
        "\n",
        "        logvar : torch.Tensor\n",
        "            Diagonal log variance of the normal distribution. Shape (batch_size,\n",
        "            latent_dim)\n",
        "\n",
        "        mode : 'sample' or 'mean'\n",
        "            Returns either a sample from qzx, or just the mean of qzx. The former\n",
        "            is useful at training time. The latter is useful at inference time as\n",
        "            the mean is usually used for reconstruction, rather than a sample.\n",
        "        \"\"\"\n",
        "        if mode=='sample':\n",
        "            # Implements the reparametrization trick (slide 43):\n",
        "            std = torch.exp(0.5 * logvar)  # Standard deviation\n",
        "            eps = torch.randn_like(std)  # Random noise\n",
        "            return mean + eps * std\n",
        "        elif mode=='mean':\n",
        "            return mean\n",
        "        else:\n",
        "            return ValueError(\"Unknown mode: {mode}\".format(mode))\n",
        "\n",
        "    def forward(self, x, mode='sample'):\n",
        "        \"\"\"\n",
        "        Forward pass of model, used for training or reconstruction.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
        "\n",
        "        mode : 'sample' or 'mean'\n",
        "            Reconstructs using either a sample from qzx or the mean of qzx\n",
        "        \"\"\"\n",
        "\n",
        "        # stats_qzx is the output of the encoder\n",
        "        stats_qzx = self.encoder(x)\n",
        "\n",
        "        # Use the reparametrization trick to sample from q(z|x)\n",
        "        samples_qzx = self.reparameterize(*stats_qzx.unbind(-1), mode=mode)\n",
        "\n",
        "        # Decode the samples to image space\n",
        "        reconstructions = self.decoder(samples_qzx)\n",
        "\n",
        "        # Return everything:\n",
        "        return {\n",
        "            'reconstructions': reconstructions,\n",
        "            'stats_qzx': stats_qzx,\n",
        "            'samples_qzx': samples_qzx}\n",
        "\n",
        "    def sample_qzx(self, x):\n",
        "        \"\"\"\n",
        "        Returns a sample z from the latent distribution q(z|x).\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        x : torch.Tensor\n",
        "            Batch of data. Shape (batch_size, n_chan, height, width)\n",
        "        \"\"\"\n",
        "        stats_qzx = self.encoder(x)\n",
        "        mean, logvar = stats_qzx[:, :self.latent_dim], stats_qzx[:, self.latent_dim:]\n",
        "        samples_qzx = self.reparameterize(mean, logvar)\n",
        "        return samples_qzx\n",
        "\n",
        "    def sample_pz(self, N):\n",
        "        samples_pz = torch.randn(N, self.latent_dim, device=self.encoder.conv1.weight.device)\n",
        "        return samples_pz\n",
        "\n",
        "    def generate_samples(self, samples_pz=None, N=None):\n",
        "        if samples_pz is None:\n",
        "            if N is None:\n",
        "                return ValueError(\"samples_pz and N cannot be set to None at the same time. Specify one of the two.\")\n",
        "\n",
        "            # If samples z are not provided, we sample N samples from the prior\n",
        "            # p(z)=N(0,Id), using sample_pz\n",
        "            samples_pz = self.sample_pz(N) # FILL IN CODE\n",
        "\n",
        "        # Decode the z's to obtain samples in image space (here, probability\n",
        "        # maps which can later be sampled from or thresholded)\n",
        "        generations = self.decoder(samples_pz) # FILL IN CODE\n",
        "        return {'generations': generations}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KL divergence term is computed as per the regularization term in slide 45 i.e., for each data sample in the mini-batch:\n",
        "$$\\frac{1}{2}\\sum_{j=1}^D (\\mu_j^2 + \\sigma_j^2 - 1 - \\log{\\sigma_j^2})$$"
      ],
      "metadata": {
        "id": "MUsrzszm-Hnf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "-pc40PPM7adL"
      },
      "outputs": [],
      "source": [
        "def kl_normal_loss(mean, logvar):\n",
        "    \"\"\"\n",
        "    Calculates the KL divergence between a normal distribution\n",
        "    with diagonal covariance and a unit normal distribution.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    mean : torch.Tensor\n",
        "        Mean of the normal distribution. Shape (batch_size, latent_dim) where\n",
        "        D is dimension of distribution.\n",
        "\n",
        "    logvar : torch.Tensor\n",
        "        Diagonal log variance of the normal distribution. Shape (batch_size,\n",
        "        latent_dim)\n",
        "    \"\"\"\n",
        "    # To be consistent with the reconstruction loss, wetake the mean over the\n",
        "    # minibatch (i.e., compute for each sample in the minibatch according to\n",
        "    # the equation above, then take the mean).\n",
        "    # Calculate each term of the KL divergence formula\n",
        "    term1 = mean.pow(2)\n",
        "    term2 = logvar.exp()\n",
        "    term3 = 1\n",
        "    term4 = logvar\n",
        "\n",
        "    # Sum terms up and average over all dimensions and then take the mean over the batch\n",
        "    latent_kl = 0.5 * torch.sum(term1 + term2 - term3 - term4, dim=1).mean()\n",
        "\n",
        "    return latent_kl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `BetaVAELoss` puts it all together as per slide 55."
      ],
      "metadata": {
        "id": "WhJbXJ0y_8OI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "x_hr2EwiCRSv"
      },
      "outputs": [],
      "source": [
        "class BetaVAELoss(object):\n",
        "    \"\"\"\n",
        "    Compute the Beta-VAE loss\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        beta: (scalar) the weight assigned to the regularization term\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, beta):\n",
        "        self.beta = beta\n",
        "\n",
        "    def __call__(self, reconstructions, data, stats_qzx):\n",
        "        mean, logvar = stats_qzx.unbind(-1)\n",
        "\n",
        "        # Reconstruction loss\n",
        "        rec_loss = reconstruction_loss(reconstructions, data)\n",
        "\n",
        "        # KL divergence loss\n",
        "        kl_loss = kl_normal_loss(mean, logvar)\n",
        "\n",
        "        # Total loss of beta-VAE\n",
        "        loss = rec_loss + self.beta * kl_loss\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Variational Autoencoder"
      ],
      "metadata": {
        "id": "3RGTCPZXWaz6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk_9fDIphlsi"
      },
      "source": [
        "This follows the traditional pipeline that you are by now familiar with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "bOdRTeDAMJCO"
      },
      "outputs": [],
      "source": [
        "latent_dim = 10\n",
        "\n",
        "learning_rate = 1e-3\n",
        "n_epoch = 5 # use the same number of epochs as before for fairness"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "xGN3jpxqOOwg",
        "outputId": "63e057b6-7d6b-4e46-975c-8c3a6c13b51d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "D5XnxIE1L1bI"
      },
      "outputs": [],
      "source": [
        "vae_model = VAEModel(latent_dim=latent_dim)\n",
        "vae_model = vae_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "fBVo5s-dQCBb"
      },
      "outputs": [],
      "source": [
        "# To keep it simple, we can leave beta at 1.0 for the beta-VAE loss\n",
        "# Feel free to experiment with it to see different trade-offs between reconstruction\n",
        "# and generation performance.\n",
        "\n",
        "beta = 1.0\n",
        "vae_loss = BetaVAELoss(beta=beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "GiqsBcP7KIT3"
      },
      "outputs": [],
      "source": [
        "# AdamW, with learning rate set to the parameter above and weight decay to 1e-4\n",
        "optimizer = optim.AdamW(vae_model.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae_model.train()\n",
        "\n",
        "for epoch in range(0,n_epoch):\n",
        "  train_loss=0.0\n",
        "\n",
        "  with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n",
        "    for data, labels in tepoch:\n",
        "        tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "        # Move data to the appropriate device\n",
        "        data = data.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Pass the input data through the VAE model\n",
        "        predict = vae_model(data)\n",
        "        reconstructions = predict['reconstructions']\n",
        "        stats_qzx = predict['stats_qzx']\n",
        "\n",
        "        # Compute the beta-VAE loss\n",
        "        loss = vae_loss(reconstructions, data, stats_qzx)\n",
        "\n",
        "        # Backpropagate the loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Aggregate the training loss for display at the end of the epoch\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # Update the tqdm bar to show the current loss\n",
        "        tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "  print('Epoch {}: Train Loss: {:.4f}'.format(epoch, train_loss/len(mnist_train_loader)))"
      ],
      "metadata": {
        "id": "z4KHRufgxNmR",
        "outputId": "e5c8b009-1e80-4d1b-ce8e-f0047f73ac52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 938/938 [00:58<00:00, 15.93batch/s, loss=139]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss: 175.2940\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 938/938 [00:58<00:00, 15.95batch/s, loss=112]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Train Loss: 125.6520\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 938/938 [00:59<00:00, 15.81batch/s, loss=125]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: Train Loss: 117.8193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 938/938 [00:58<00:00, 15.93batch/s, loss=115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: Train Loss: 114.0299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 938/938 [00:59<00:00, 15.86batch/s, loss=113]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: Train Loss: 111.7044\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the VAE model"
      ],
      "metadata": {
        "id": "3DFW0vPRXrSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check how well the VAE reconstructs training samples:"
      ],
      "metadata": {
        "id": "Who7o-hAXLhl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "NiRg43Bgx_MH",
        "outputId": "a454743e-ec51-4036-8cc2-a36bc50193be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# reconstructing training images\n",
        "train_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(vae_model, train_imgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Same for test samples:"
      ],
      "metadata": {
        "id": "Bqa9wmZ3XSxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reconstructing test images\n",
        "test_imgs = next(iter(mnist_test_loader))[0]\n",
        "display_ae_images(vae_model, test_imgs)"
      ],
      "metadata": {
        "id": "IRJ9-V0tx_MJ",
        "outputId": "adf990df-9541-47cd-f186-7f9bc517c053",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute the quantitative `reconstruction_loss` on the test data:"
      ],
      "metadata": {
        "id": "OR-uGTX2XaEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !!! If you copy paste code, don't forget to change ae_model to vae_model !!!\n",
        "\n",
        "# FILL IN CODE\n",
        "\n",
        "# Visual evaluation: Reconstruct training images\n",
        "train_imgs = next(iter(mnist_train_loader))[0]\n",
        "display_ae_images(vae_model, train_imgs)\n",
        "\n",
        "# Visual evaluation: Reconstruct test images\n",
        "test_imgs = next(iter(mnist_test_loader))[0]\n",
        "display_ae_images(vae_model, test_imgs)\n",
        "\n",
        "# Quantitative evaluation: Compute reconstruction loss on the test data\n",
        "vae_model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "test_loss = 0.0\n",
        "n_samples = 0\n",
        "\n",
        "with torch.no_grad():  # Inference only, no gradients needed\n",
        "    for data, _ in tqdm(mnist_test_loader, desc=\"Computing test loss\", unit=\"batch\"):\n",
        "        data = data.to(device)\n",
        "        output = vae_model(data)\n",
        "        reconstructions = output['reconstructions']\n",
        "\n",
        "        loss = reconstruction_loss(reconstructions, data)\n",
        "        test_loss += loss.item() * data.size(0)\n",
        "        n_samples += data.size(0)\n",
        "\n",
        "average_test_loss = test_loss / n_samples\n",
        "print(f\"Average Reconstruction Loss on Test Data: {average_test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "P0EzdVDzx_MJ",
        "outputId": "22587492-c1bb-4d09-904b-8d13d6fe966b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        }
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing test loss: 100%|██████████| 157/157 [00:04<00:00, 37.38batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Reconstruction Loss on Test Data: 93.0015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image generation with the VAE model"
      ],
      "metadata": {
        "id": "wFOJ0yCzXuxO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTfRje_AkKDr"
      },
      "source": [
        "Now, generate some images with the VAE model. You can directly use the `generate_samples` routine from the `VAEModel` class above."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_images_vae(vae_model, n_images=5):\n",
        "    # Use the generate_samples method from VAEModel to sample and decode images\n",
        "    generated_data = vae_model.generate_samples(N=n_images)\n",
        "\n",
        "    # The generated images are stored under the key 'generations'\n",
        "    generated_images = generated_data['generations']\n",
        "\n",
        "    return generated_images"
      ],
      "metadata": {
        "id": "NGt_LSDEE2vz"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "41tXdNsFkKk5",
        "outputId": "41d5f4b7-b551-4b3c-fb50-7057f3375144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 5 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "imgs_generated = generate_images_vae(vae_model)\n",
        "\n",
        "display_images(imgs_generated)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGnvKoynzaFN"
      },
      "source": [
        "Do you think the results are better ? What advantage does the Variational Autoencoder have over the simple autoencoder model, even though the second (non-diagonal Gaussian model) AE approach has a more complex probabilistic latent model (a full covariance matrix)?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "The Variational Autoencoder (VAE) generally offers superior performance and capabilities over a simple Autoencoder (AE), particularly for tasks involving the generation of new data. The VAE models the latent space as a probabilistic distribution, inherently supporting the generation of diverse and realistic images by sampling from this distribution. This is fundamentally integrated into the training process through the minimization of both reconstruction loss and KL divergence, ensuring that the latent space is informative for reconstruction while adhering to a controlled distribution.\n",
        "\n",
        "In contrast, while an AE with a non-diagonal Gaussian model attempts to enhance generation capabilities by estimating complex latent distributions post hoc, it lacks the VAE's integrated approach and theoretical backing. This often results in a latent space that is optimal for reconstruction but less effective for generation. The VAE's structured approach, including mechanisms to balance reconstruction fidelity with latent space regularization (such as the β parameter in Beta-VAEs), provides greater flexibility and control, making it better suited for generating new, high-quality samples."
      ],
      "metadata": {
        "id": "_5mwEKeGR0em"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXm-D9Ef9vYm"
      },
      "source": [
        "Next, we compare the models quantitavely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04MddkzuE324"
      },
      "source": [
        "# 4. Evaluating and comparing the models for image generation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "D8WsuVqmaWmd",
        "outputId": "eede2108-2b8d-40e2-840c-c2358581d1ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nowadays, the standard metric to evaluate the generative performance of a model is the FID (Fréchet Inception Distance). I leave it for you as optional homework to implement it if you wish to do so.\n",
        "\n",
        "Here, we will follow another path, a simplified version of the Inception Score (IS) that has been somewhat superseded by the FID:\n",
        "\n",
        "- we train a simple convolutional neural network classifier on MNIST, to a good accuracy\n",
        "- we generate images with each model\n",
        "- we find the average of the highest probability of the images according to the classifier, for each model. If this value is high, it means that on average the classifier considers that the images looks like a real image of an actual digit"
      ],
      "metadata": {
        "id": "B93T6PD2YhgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the following simple convolutional architecture for the classifier:\n",
        "\n",
        "- conv2d, 3x3 kernel, 32 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n",
        "- conv2d, 3x3 kernel, 64 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n",
        "- conv2d, 3x3 kernel, 128 filters, stride=2, padding=1; + ReLU + BatchNorm2d\n",
        "- Global Average Pooling\n",
        "- Flatten\n",
        "- Dense layer\n",
        "\n",
        "Now, define the model. To make things easier, we use the `torch.nn.Sequential` API to implement the model (there is no need for a Class in this simple case).\n",
        "\n",
        "__Hint__. For the global average pooling, use the `torch.nn.AvgPool2d` layer with the suitable kernel size and stride."
      ],
      "metadata": {
        "id": "Fk1OdN6jZZf1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "P87a-DkXFOCv"
      },
      "outputs": [],
      "source": [
        "nb_classes = 10\n",
        "kernel_size = (3, 3)\n",
        "stride = 2\n",
        "padding = 1\n",
        "\n",
        "mnist_classification_model = torch.nn.Sequential(\n",
        "    # First convolutional layer\n",
        "    nn.Conv2d(in_channels=1, out_channels=32, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(32),\n",
        "\n",
        "    # Second convolutional layer\n",
        "    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(64),\n",
        "\n",
        "    # Third convolutional layer\n",
        "    nn.Conv2d(in_channels=64, out_channels=128, kernel_size=kernel_size, stride=stride, padding=padding),\n",
        "    nn.ReLU(),\n",
        "    nn.BatchNorm2d(128),\n",
        "\n",
        "    # Global Average Pooling\n",
        "    nn.AdaptiveAvgPool2d(1),\n",
        "\n",
        "    # Flatten the output for the dense layer\n",
        "    nn.Flatten(),\n",
        "\n",
        "    # Dense layer to classify the images into 10 classes (for the 10 digits)\n",
        "    nn.Linear(128, nb_classes)\n",
        ")\n",
        "\n",
        "# Move the model to the appropriate device (GPU or CPU)\n",
        "mnist_classification_model = mnist_classification_model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-2\n",
        "n_epoch = 5"
      ],
      "metadata": {
        "id": "rJ6ZYkKVaY64"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross entropy with reduction='sum'\n",
        "criterion = nn.CrossEntropyLoss(reduction='sum')"
      ],
      "metadata": {
        "id": "Zexnteq71bJ5"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# AdamW, weight decay set to 1e-4\n",
        "optimizer = optim.AdamW(mnist_classification_model.parameters(), lr=learning_rate, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "pWkFh6zHajBg"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vector_to_class(x):\n",
        "  y = torch.argmax(F.softmax(x,dim=1),axis=1)\n",
        "  return y"
      ],
      "metadata": {
        "id": "eBZrmVV42Gej"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "aOww0ydr2fT0"
      },
      "outputs": [],
      "source": [
        "def cnn_accuracy(x_pred,x_label):\n",
        "  acc = (x_pred == x_label).sum()/(x_pred.shape[0])\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "0FA8YoX2FcHP",
        "outputId": "5b0097c8-ea42-4a21-9fd4-f0a0ec99b4ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 938/938 [00:53<00:00, 17.61batch/s, loss=1.92]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:0 Train Loss:0.1661 Accuracy:0.9488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 938/938 [00:55<00:00, 16.81batch/s, loss=0.261]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:1 Train Loss:0.0615 Accuracy:0.9805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 938/938 [00:52<00:00, 17.80batch/s, loss=2.2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:2 Train Loss:0.0475 Accuracy:0.9852\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 938/938 [00:55<00:00, 16.84batch/s, loss=0.0427]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:3 Train Loss:0.0383 Accuracy:0.9872\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 938/938 [00:52<00:00, 17.89batch/s, loss=0.0851]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:4 Train Loss:0.0312 Accuracy:0.9897\n"
          ]
        }
      ],
      "source": [
        "mnist_classification_model.train()\n",
        "\n",
        "for epoch in range(0, n_epoch):\n",
        "    train_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predicted = []\n",
        "\n",
        "    with tqdm(mnist_train_loader, unit=\"batch\") as tepoch:\n",
        "        for imgs, labels in tepoch:\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "\n",
        "            # Put data on the correct device\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Zero the gradients before running the forward pass\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass to get the output logits\n",
        "            predict = mnist_classification_model(imgs)\n",
        "\n",
        "            # Compute the loss\n",
        "            loss = criterion(predict, labels)\n",
        "\n",
        "            # Backpropagation to compute gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Update the model weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # Compute the train loss (sum of losses over the batch)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Store labels and class predictions for accuracy computation\n",
        "            all_labels.extend(labels.cpu().tolist())\n",
        "            all_predicted.extend(vector_to_class(predict).cpu().tolist())\n",
        "\n",
        "            # tqdm bar displays the loss\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "\n",
        "    # Calculate overall accuracy\n",
        "    accuracy = cnn_accuracy(torch.tensor(all_predicted), torch.tensor(all_labels))\n",
        "    print(f'Epoch:{epoch} Train Loss:{train_loss / len(mnist_train_loader.dataset):.4f} Accuracy:{accuracy:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_classification_model.eval()\n",
        "\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with tqdm(mnist_test_loader, unit=\"batch\") as tepoch:\n",
        "  for imgs, labels in tepoch:\n",
        "    all_labels.extend(labels.tolist())\n",
        "\n",
        "    imgs = imgs.to(device)\n",
        "    predict=mnist_classification_model(imgs)\n",
        "    all_predicted.extend(vector_to_class(predict).tolist())\n",
        "\n",
        "test_accuracy = cnn_accuracy(np.array(all_predicted),np.array(all_labels))\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "5bRM-1_x4bu2",
        "outputId": "64619889-468b-461b-b182-b0abacfd1fb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:03<00:00, 50.87batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.9836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFgN5LblFwTa"
      },
      "source": [
        "### Evaluate the average maximum prediction of the images generated by each generative model (higher is better)\n",
        "\n",
        "Now, we evaluate the models. For each ones, produce a certain number of images, and put those images through the classification network. Then find the maximum class probability of each image, and average it over all the images. We will use this as a metric to evaluate each model.\n",
        "\n",
        "__CAREFUL__: the output of the network does __not__ include the Softmax layer, so you will have to carry it out, with:\n",
        "- ```torch.nn.Softmax()(...)```\n",
        "\n",
        "Define this metric now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "lCJ_0qqjOXHT"
      },
      "outputs": [],
      "source": [
        "def generative_model_score(imgs_in, classification_model):\n",
        "    # Move images to the appropriate device\n",
        "    imgs_in = imgs_in.to(device)\n",
        "\n",
        "    # Get the predictions from the classification model\n",
        "    predictions = classification_model(imgs_in)\n",
        "\n",
        "    # Apply softmax to convert logits to probabilities\n",
        "    probabilities = F.softmax(predictions, dim=1)\n",
        "\n",
        "    # Get the maximum probability for each image\n",
        "    max_probs = probabilities.max(dim=1)[0]\n",
        "\n",
        "    # Calculate the average of these maximum probabilities\n",
        "    gen_score = max_probs.mean().item()\n",
        "    return gen_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGq7YFg51UoP"
      },
      "source": [
        "Now, generate some images with each of the three models, and evaluate these models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "4-L4u2jhILFx",
        "outputId": "f804c5b6-aa8f-4d64-8818-36673149bf6a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diagonal gaussian generative model score :  0.8688862323760986\n",
            "Non diagonal gaussian generative model score :  0.8999066948890686\n",
            "Variational autoencoder model score:  0.8990873098373413\n"
          ]
        }
      ],
      "source": [
        "imgs_diagonal_gaussian = generate_images_diagonal_gaussian(ae_model,z_average,z_sigma,n_images = 2000)\n",
        "imgs_non_diagonal_gaussian = generate_images_non_diagonal_gaussian(ae_model,z_average,L,n_images = 2000)\n",
        "imgs_vae = generate_images_vae(vae_model,n_images=2000)\n",
        "\n",
        "# average of maximum of first model\n",
        "diagonal_gaussian_score = float(generative_model_score(imgs_diagonal_gaussian,mnist_classification_model))\n",
        "non_diagonal_gaussian_score = float(generative_model_score(imgs_non_diagonal_gaussian,mnist_classification_model))\n",
        "vae_gaussian_score = float(generative_model_score(imgs_vae,mnist_classification_model))\n",
        "\n",
        "print(\"Diagonal gaussian generative model score : \",diagonal_gaussian_score)\n",
        "print(\"Non diagonal gaussian generative model score : \",non_diagonal_gaussian_score)\n",
        "print(\"Variational autoencoder model score: \",vae_gaussian_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxvsG8FC1gNS"
      },
      "source": [
        "Questions:\n",
        "\n",
        "- Which model is better quantitatively ? (unfortunately there is some variability, even with 2000 samples; you might want to rerun the cell several times to get the trend)\n",
        "- Do the quantitative result support the qualitative results ?\n",
        "- Can you see any drawbacks of this method of evaluation ?\n",
        "- Can you propose any more sophisticated models than the multivariate Gaussian approach (apart from the variational autoencoder) ?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:**\n",
        "\n",
        "1. **Which model is better quantitatively?**\n",
        "   - Quantitatively, the model with the non-diagonal Gaussian generative model has a slightly higher score (0.8999) compared to the Variational Autoencoder (0.8991) and the diagonal Gaussian model (0.8689). This suggests that, for this particular run, the non-diagonal Gaussian model performed the best. However, the difference between the non-diagonal Gaussian model and the VAE is quite small, indicating similar performance. Multiple runs are necessary to discern a clear trend due to inherent variability.\n",
        "\n",
        "2. **Do the quantitative results support the qualitative results?**\n",
        "   - If the qualitative results (visual inspection of generated images) show that images from the non-diagonal Gaussian model and the VAE look more realistic and diverse compared to those from the diagonal Gaussian model, then the quantitative results do support the qualitative ones. Otherwise, if the visual quality does not match these scores, it may suggest limitations in how the evaluation metric aligns with human perception.\n",
        "\n",
        "3. **Drawbacks of this method of evaluation:**\n",
        "   - The primary drawback of using classifier-based metrics like these scores is their heavy reliance on the classifier's performance and biases. If the classifier is not well-generalized, it may not accurately reflect the true quality of the images. Additionally, this method evaluates realism but not diversity or novelty, which are also important aspects of generative models.\n",
        "\n",
        "4. **More sophisticated models:**\n",
        "   - Beyond the multivariate Gaussian and VAE approaches, other sophisticated models include:\n",
        "     - **Generative Adversarial Networks (GANs)**: Known for producing high-quality images by training two networks in opposition to each other.\n",
        "     - **Normalizing Flows**: Provides exact log-likelihood computation and reversible transformations, suitable for detailed and diverse sample generation.\n",
        "     - **Autoregressive Models** like PixelCNN, which generate images pixel by pixel, ensuring high levels of detail and coherence.\n",
        "     - **Energy-Based Models (EBMs)**: Learn an energy landscape where lower energies are more probable, allowing generation of diverse samples by navigating this landscape.\n",
        "\n",
        "Each of these models offers distinct advantages in handling complex distributions, providing diversity, and achieving realistic image generation, potentially surpassing simpler Gaussian-based methods in various aspects."
      ],
      "metadata": {
        "id": "NMeLsVIwVMKU"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}